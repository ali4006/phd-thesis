\chapter{Discussion}
\label{ch:discussion}
\renewcommand\thesection{\@arabic\c@chapter.\@arabic\c@section}

The aim of this thesis was to study the numerical stability of neuroimaging pipelines focusing on the effect
of operating system variations. We leveraged system call interception techniques,
the ReproZip tool and perturbation models such as Monte-Carlo arithmetic (MCA)~\cite{Parker1997-qq}. 
% as an extension of standard floating-point arithmetic that exploits randomness in basic floating-point operations.
We showed that \dots.
This chapter first discusses the findings and contributions.
It then discuss the implication of the results and limitations of the current methodology.
This chapter concludes with recommendations for future studies.


% \subsection{Thesis findings}
\section{The Impact of Numerical Perturbations}

We showed the role of numerical errors in different computational environments in neuroimaging pipelines.
In Chapter~\ref{ch:c1}, we introduced Spot, a tool to detect the source of numerical differences in pipelines
executed on different operating systems. 
This work localized the origin of processes that hamper bitwise reproducibility due to OS updates.
We observed registration processes as the highly contributing source of instability in the FSL tool. 
We found that differences can propagate during the pipeline execution and substantially impact
the results of the entire pipeline (e.g., segmentations created by FreeSurfer).
We also obtained different results between subjects, showing the sensitivity of the analyses to dataset selection.
The irreproducible processes identified by Spot were all associated with dynamically linked executables.
% showing that library updates do not impact statically linked executables.


The numerical differences were caused by truncation and round-off errors
due to the precision limitations of the floating-point arithmetic.
This is uncontrolled error that originated from the updates of the system libraries.
The observed instability across OSes
depends on which operating systems are used. This limits the evaluation of the underlying stability of a particular tool.

In Chapter~\ref{ch:c2}, we presented a framework to model the numerical uncertainty induced by OS updates in a control condition using Monte-Carlo arithmetic. 
We obtained an accurate simulation of OS variations using the fuzzy libmath library,
a version of the GNU mathematical library (libmath) instrumented with Monte-Carlo arithmetic.
% This suggests that numerical perturbations introduced by OS updates might be in the range of machine error (virtual precision of 53 bits).
We found that the pipeline could be implemented exclusively with lower precision of floating-point representations (single-precision)
without loss of results precision. This would substantially decrease the pipeline memory footprint and computational time.
However, the finding showed a very low number of significant digits, 5 out of 15 available digits.
This motivates further investigation of the numerical stability of the main components of the pipelines, such as linear and non-linear registrations.
Also, it is notable that OS- and FL-induced variability were on a similar order of magnitude as subject-level effects,
showing that we applied a reasonable amount of perturbations.

This framework works on shared libraries, so there is no need for recompilation or modification
of the pipeline or any other sources.
We believe that pipelines that depend
on different third-party mathematical libraries could be studied similarly by building fuzzy
versions of these dependencies. This motivated us to investigate numerical quality in
more applications using the proposed MCA-based method.

Finally, in Chapter~\ref{ch:c3}, we investigated the changes to pipeline results due to variations between software packages.
We compared numerical variability with tool variability across three of the most popular software packages in neuroimaging.
% This work presents a comparison of numerical and software variability for group-level functional MRI analysis
% We found significant numerical variability that was comparable to tool variability in some brain regions.
In fMRI group analyses, numerical variability was found to be an order of magnitude smaller than tool variability.
We also found that numerical instability in individual analyses was attenuated in group analyses.
We obtained more uncertainty on thresholded results than unthresholded maps, probably due to
different thresholds used in different tools.
% This raises further investigations on thresholding techniques to reduce the associated instabilities.


% \subsection{The implication of results}
\section{The Importance of Numerical Instability}

This study was a contribution to uncertainty quantification in medical imaging.
Numerical errors are often neglected or only partially studied due to the associated engineering challenges among
the various sources of uncertainty involved in medical imaging results, including population selection,
scanning devices and sequence parameters, acquisition noise, image reconstruction algorithms, and methodological flexibility.
Our work proposed methodologies and implementations to address this issue.

The current approach to address numerical instability resulting from OS
updates is mainly to ignore the issue and hide it using %static build of programs, or 
Docker containers or other types of virtualization.
Although building static program and containerization techniques improve
reproducibility across OSes, but small differences remained.
It remains that computational results should be understood as realizations of a random variable resulting
from floating-point arithmetic. The presented techniques in this thesis enable estimating
result distributions, the initial step toward making analyses reproducible across
execution environments, including HPC systems, GPU accelerators, or merely
different workstations.

Results showed the significance of numerical instabilities in neuroimaging pipelines
and demonstrated numerical analysis techniques such as MCA as valuable methods for evaluating the associated variability. 
Moreover, a related work~\cite{kiar2020numerical} suggested that capturing this variability may improve the robustness of scientific
findings. This finding highlights how numerical variability may be a feature that should be
taken into considerations by the pipeline developers.

We demonstrated the numerical noise sensitivity of two of the most complex pipelines in neuroimaging,
HCP preprocessing pipelines, PreFreeSurfer and FreeSurfer. These pipelines consist of a mix of tools
assembled from different toolboxes through a variety of scripts written in different languages. In addition to the preprocessing steps,
we evaluated the numerical stability of a complete fMRI analysis using three of the most popular tools in neuroimaging, FSL, SPM, and AFNI.
Thus, the results presented in this thesis would apply to a wide range of other pipelines. 
However, the current methodology is limited to Linux operating systems. 
Our findings are likely to generalize to OS/X or MS Windows, although future work would be needed to confirm that.

Moreover, the interposition technique is only applicable to intercept system calls in dynamically linked programs. 
Further investigations are needed to evaluate the stability of pipelines with statically linked executables.


\section{Recommendations for Future Research}

Among the processes that contribute to pipeline instability, some of them substantially
have a higher effect on results. For instance, iterative computations accumulate rounding errors
and significantly amplify these errors. 
Considering this, the next steps in this area could be evaluating the stability of pipeline
components as well as the entire pipeline. This would help to identify processes that propagate
and amplify errors and then substitute them for more stable tools that perform a similar function,
ultimately improving the quality of the pipeline.
It is important to propose numerical debugging tools to identify such numerical bugs during the
execution of pipelines. Numreical errors are a well-studied problem in the floating-point error characterization
research field, which has resulted in the development of debugging tools such as VeriTracer~\cite{chatelain2018veritracer},
FpDebug~\cite{benz2012dynamic}, Verrou~\cite{fevotte2019debugging}.
These tools automatically replace all the floating-point operations with their MCA counterparts
or other stochastic arithmetics to evaluate the numerical quality of the computation and pinpoint the parts
of the source code.
They instrument pipelines at compile time or runtime using Valgrind-based tools,
which have limitations, including the need to access the source code, pre-knowledge of
the functions or variables in the pipeline, and computation time overhead.

Using a combination of techniques developed in this thesis, in future work,
we can create a debugging tool at the level of system call processes without
recompiling the source code to identify the processes with the highest impact on results in the pipeline due
to the numerical errors. 
This functionality could leverage the tool
developed in Chapter~\ref{ch:c1} and the interposition technique to inject MCA perturbations described in Chapter~\ref{ch:c2}.
Moreover, we could use the principle of minimization by delta-debugging to search through a large number of processes. 
The delta-debugging algorithm is a general technique that can automatically narrow downthe failure caused by changing circumstances
that are critical to producing a bug, such as program input, program code, environmental configurations,
etc.~\cite{zeller1999yesterday,zeller2002isolating}. Instead of working on the source code, we can apply delta-debugging to the
program history by comparing various versions until the faulty change is found. Finding these processes could
narrow down further investigations toward stabilizing the pipelines.

Improving stability needs actions different from evaluation and localization of instability.
There is not a general approach to stabilize the entire pipeline; we should look at the relevant code section
to find an appropriate solution. For example, we found that linear and non-linear registration processes introduce errors
in both FreeSurfer and PreFreeSurfer pipelines. We know that the registration procedure is sensitive to
the initialization of the optimization method used~\cite{Glatard2018hbm}. 
A possible method to address such instabilities is using the bootstrapping technique.
In~\cite{Glatard2018hbm}, the authors explained that bootstrapping is an efficient technique to improve
the robustness of motion estimation. The bootstrap version of the pipelines computed the median transformation
results from the 30 samples. In addition to bootstrapping, it is shown that the bagging technique can
reduce the effect of the medians of the parameters of the 30 transformations. Bagging, also called bootstrap
aggregating, is a simple and powerful ensemble method to reduce both bias and variance in the results.
So, we can possibly stabilize pipelines and improve their accuracy using aggregates of results obtained with
data perturbations. However, it is a compute-intensive technique that adds a significant computation time overhead,
so that should be used only when no other solution to stability is available.  
Further study must be conducted into the processes involved in analysis instability.

% The main goal of this thesis was to evaluate instabilities within neuroimaging pipelines. 
An exciting research topic for future work could be finding that the variability arising from numerical perturbations may contain
meaningful signals. The perturbation model is not only helpful as a method for measuring
the stability of analyses but that it can improve their quality. 
We can also apply other types of perturbations.
Given that FL only perturbs basic mathematical functions, we expect more numerical variability
by perturbing operations that rely on the linear algebra libraries BLAS and LAPACK.
In future work, this can be evaluated using MCA-instrumented
versions of BLAS and LAPACK along with other libraries available in the Fuzzy project in Verificarloâ€™s
GitHub repository at \url{github.com/verificarlo/fuzzy}.

\section{Conclusion}

The numerical stability of computational analyses plays an important role in the reproducibility
of scientific findings. These analyses are sensitive to the
computing environment changes such as operating system and analysis toolboxes, particularly
in computationally intensive domains where results rely on a series of complex computations.
Throughout this thesis, we demonstrated the impact of numerical instabilities on neuroimaging results. 
We implemented Spot, a tool that localizes irreproducible processes between operating system variations.
Moreover, we presented an MCA-based method to apply numerical perturbations in floating-point operations,
simulating OS variability and studying the numerical instabilities more comprehensively.
We used the Linux interception utility LD\_PRELOAD,
which transparently interposes system calls to an instrumented counterpart.
We expanded our findings by capturing numerical variability among different software packages and comparing 
software variability across and within fMRI analysis packages.
As the successful completion of this thesis, we built freely available tools that enable software developers
and researchers to evaluate the stability of their pipelines and results when dynamically
linked mathematical libraries are changed. The findings of this thesis could be
used to narrow down further investigations toward stabilizing pipelines.

% More effort in the conclusion as to what did he achieve, how it affects the world, etc